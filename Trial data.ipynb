{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe46c39-27c6-40f2-bbea-3ace6884466e",
   "metadata": {},
   "source": [
    "## TODO/uncertainties\n",
    "\n",
    "* Do we only care about nth-phase studies? Or is any study that progresses promising for stock price?\n",
    "* If we need to run the query repeatedly rather than one mammoth operation, refine the process (e.g. use `fields` query param to reduce what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e08aad58-c6c0-4fee-bb6c-582fb900110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_clinical_trials(page_token=None):\n",
    "    # Base URL for the API\n",
    "    url = \"https://clinicaltrials.gov/api/v2/studies?aggFilters=results%3Awith&sort=LastUpdatePostDate\"\n",
    "    \n",
    "    if page_token:\n",
    "        url = url + f\"&pageToken={page_token}\"\n",
    "    \n",
    "    try:\n",
    "        # Make the GET request\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Raise an exception for bad status codes\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse JSON response into a dictionary\n",
    "        data = response.json()\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return None\n",
    "    \n",
    "trials = get_clinical_trials() \n",
    "trials2 = get_clinical_trials(trials['nextPageToken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "567a5a8a-290c-4404-a736-c00941683ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a datetime object\n",
    "def convert_date(date_string):\n",
    "    return datetime.strptime(date_string, '%Y-%m-%d').date()\n",
    "\n",
    "def trial_date(trial):\n",
    "    date_string = trial['protocolSection']['statusModule']['lastUpdatePostDateStruct']['date']\n",
    "    return convert_date(date_string)\n",
    "\n",
    "def org_info(trial):\n",
    "    \"\"\"Returns dict with informal(?) org name, 'fullName' and 'type'\n",
    "    where type\"\"\"\n",
    "    return trial['protocolSection']['identificationModule']['organization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0e5ee011-581d-4336-91d1-83c79fce6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_trials(min_date=None):\n",
    "    \"\"\"Repeatedly hits the clinicaltrials.gov API to retrieve everything on their db up to some min date. \n",
    "    Unknown how long it takes to fetch 10 years worth - for regular use, we would need to refine the process, \n",
    "    and we might need to chunk it to get and process all the data even once.\n",
    "    (if their sorting is reliable we could limit by max date rather than number, but\n",
    "    I wouldn't be inclined to trust it without understanding it more deeply than I have time to here).\"\"\"\n",
    "    trials_list = []\n",
    "    trials = get_clinical_trials()\n",
    "    memos = []\n",
    "    min_date = convert_date(min_date)\n",
    "    page_date = datetime.today().date()\n",
    "    while trials['nextPageToken'] and trials['nextPageToken'] not in memos and not min_date or min_date <= page_date:\n",
    "        memos.append(trials['nextPageToken'])\n",
    "        trials = get_clinical_trials(trials['nextPageToken'])\n",
    "        trials_list = trials_list + trials['studies']\n",
    "        page_date = trial_date(trials['studies'][-1])\n",
    "        # print(trial_date(trials['studies'][-1]))\n",
    "        # print(f\"Study id: {trials['studies'][-1]['protocolSection']['identificationModule']['nctId']}\")\n",
    "        # print('***********')\n",
    "    return trials_list\n",
    "        \n",
    "all_trials = fetch_all_trials(min_date='2024-12-19')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fe11c11d-6c05-48b8-a2db-27cad5352e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials['nextPageToken']\n",
    "# trials['studies'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0113e8-10de-4a3b-9519-f8dfcee70655",
   "metadata": {},
   "source": [
    "## Study metrics\n",
    "\n",
    "The trials don't have an obvious 'success/failure' mark, so I provisionally use a Claude-generated scoring system based on common data among the trials. \n",
    "\n",
    "For serious use, these metrics would need to be substantially refined - we might want determine optimal metrics by an ML process that compares against stock prices, along with some basic sanity measures, like 'if more than n% of the people taking the treatment died, we mark it as an automatic failure'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85a0cac3-8c80-4d7d-a545-179e9f30c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(val, default=0.0):\n",
    "    \"\"\"Safely convert value to float\"\"\"\n",
    "    try:\n",
    "        return float(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "def safe_int(val, default=0):\n",
    "    \"\"\"Safely convert value to int\"\"\"\n",
    "    try:\n",
    "        return int(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "def score_primary_outcome(trial_data):\n",
    "    \"\"\"Score primary outcome achievement (max 0.4)\"\"\"\n",
    "    try:\n",
    "        outcomes = trial_data['resultsSection']['outcomeMeasuresModule']['outcomeMeasures']\n",
    "        primary_outcomes = [o for o in outcomes if o['type'] == 'PRIMARY']\n",
    "        \n",
    "        if not primary_outcomes:\n",
    "            return 0.2\n",
    "            \n",
    "        total_score = 0\n",
    "        total_measures = 0\n",
    "        \n",
    "        for outcome in primary_outcomes:\n",
    "            if 'analyses' in outcome and outcome['analyses']:\n",
    "                noninferior_analyses = [a for a in outcome['analyses'] \n",
    "                                      if a.get('nonInferiorityType') == 'NON_INFERIORITY']\n",
    "                if noninferior_analyses:\n",
    "                    met_criteria = 0\n",
    "                    for analysis in noninferior_analyses:\n",
    "                        if 'ciLowerLimit' in analysis and 'nonInferiorityComment' in analysis:\n",
    "                            lower_bound = safe_float(analysis['ciLowerLimit'])\n",
    "                            comment = analysis['nonInferiorityComment']\n",
    "                            try:\n",
    "                                margin = float(''.join(c for c in comment if c.isdigit() or c in '.-'))\n",
    "                                if lower_bound > margin:\n",
    "                                    met_criteria += 1\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                    total_score += met_criteria / len(noninferior_analyses)\n",
    "                    total_measures += 1\n",
    "            \n",
    "            elif 'classes' in outcome and outcome['classes']:\n",
    "                for class_data in outcome['classes']:\n",
    "                    if 'categories' in class_data:\n",
    "                        for category in class_data['categories']:\n",
    "                            if 'measurements' in category:\n",
    "                                value = safe_float(category['measurements'][0]['value'])\n",
    "                                total_score += 1 if value < 10 else value/20\n",
    "                                total_measures += 1\n",
    "\n",
    "        return min(0.4, (total_score / max(1, total_measures)) * 0.4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scoring primary outcome: {e}\")\n",
    "        return 0.2\n",
    "\n",
    "def score_safety(trial_data):\n",
    "    \"\"\"Score safety outcomes (max 0.3)\"\"\"\n",
    "    try:\n",
    "        safety_score = 0.3\n",
    "        if 'adverseEventsModule' in trial_data.get('resultsSection', {}):\n",
    "            events = trial_data['resultsSection']['adverseEventsModule']\n",
    "            \n",
    "            if 'eventGroups' in events:\n",
    "                for group in events['eventGroups']:\n",
    "                    deaths = safe_int(group.get('deathsNumAffected', 0))\n",
    "                    if deaths > 0:\n",
    "                        safety_score -= 0.1\n",
    "                    \n",
    "                    serious = safe_int(group.get('seriousNumAffected', 0))\n",
    "                    if serious > 0:\n",
    "                        safety_score -= 0.1\n",
    "                    \n",
    "                    other_affected = safe_int(group.get('otherNumAffected', 0))\n",
    "                    other_at_risk = safe_int(group.get('otherNumAtRisk', 1))\n",
    "                    \n",
    "                    if other_affected > 0 and other_at_risk > 0:\n",
    "                        rate = other_affected / other_at_risk\n",
    "                        if rate > 0.1:\n",
    "                            safety_score -= 0.1\n",
    "        \n",
    "        return max(0, safety_score)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scoring safety: {e}\")\n",
    "        return 0.15\n",
    "\n",
    "def score_study_execution(trial_data):\n",
    "    \"\"\"Score study execution (max 0.2)\"\"\"\n",
    "    try:\n",
    "        execution_score = 0\n",
    "        \n",
    "        if 'participantFlowModule' in trial_data.get('resultsSection', {}):\n",
    "            flow = trial_data['resultsSection']['participantFlowModule']\n",
    "            if 'periods' in flow and flow['periods']:\n",
    "                period = flow['periods'][0]\n",
    "                started = 0\n",
    "                completed = 0\n",
    "                \n",
    "                for milestone in period.get('milestones', []):\n",
    "                    if milestone['type'] == 'STARTED':\n",
    "                        for achievement in milestone['achievements']:\n",
    "                            started += safe_int(achievement.get('numSubjects', 0))\n",
    "                    elif milestone['type'] == 'COMPLETED':\n",
    "                        for achievement in milestone['achievements']:\n",
    "                            completed += safe_int(achievement.get('numSubjects', 0))\n",
    "                \n",
    "                if started > 0:\n",
    "                    completion_rate = completed / started\n",
    "                    if completion_rate >= 0.85:\n",
    "                        execution_score += 0.1\n",
    "                    else:\n",
    "                        execution_score += (completion_rate / 0.85) * 0.1\n",
    "\n",
    "        execution_score += 0.1  # Protocol adherence score\n",
    "        \n",
    "        return execution_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error scoring execution: {e}\")\n",
    "        return 0.1\n",
    "\n",
    "def score_secondary_outcomes(trial_data):\n",
    "    \"\"\"Score secondary outcomes (max 0.1)\"\"\"\n",
    "    try:\n",
    "        outcomes = trial_data['resultsSection']['outcomeMeasuresModule']['outcomeMeasures']\n",
    "        secondary_outcomes = [o for o in outcomes if o['type'] == 'SECONDARY']\n",
    "        \n",
    "        if not secondary_outcomes:\n",
    "            return 0.05\n",
    "            \n",
    "        total_score = 0\n",
    "        total_measures = 0\n",
    "        \n",
    "        for outcome in secondary_outcomes:\n",
    "            if 'classes' in outcome and outcome['classes']:\n",
    "                for class_data in outcome['classes']:\n",
    "                    if 'categories' in class_data:\n",
    "                        for category in class_data['categories']:\n",
    "                            if 'measurements' in category:\n",
    "                                total_measures += 1\n",
    "                                value = safe_float(category['measurements'][0]['value'])\n",
    "                                total_score += 1 if value < 10 else value/20\n",
    "\n",
    "        return min(0.1, (total_score / max(1, total_measures)) * 0.1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scoring secondary outcomes: {e}\")\n",
    "        return 0.05\n",
    "\n",
    "def score_clinical_trial(trial_data):\n",
    "    \"\"\"\n",
    "    Score a clinical trial from 0 (complete failure) to 1 (complete success)\n",
    "    \n",
    "    Parameters:\n",
    "    trial_data (dict): Clinical trial data in JSON format\n",
    "    \n",
    "    Returns:\n",
    "    dict: Score between 0 and 1 plus breakdown of scoring components\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate component scores\n",
    "        primary_score = score_primary_outcome(trial_data)\n",
    "        safety_score = score_safety(trial_data)\n",
    "        execution_score = score_study_execution(trial_data)\n",
    "        secondary_score = score_secondary_outcomes(trial_data)\n",
    "        \n",
    "        # Calculate total score\n",
    "        total_score = primary_score + safety_score + execution_score + secondary_score\n",
    "        \n",
    "        # Return both total and breakdown\n",
    "        return {\n",
    "            'total_score': round(total_score, 3),\n",
    "            'components': {\n",
    "                'primary_outcome': round(primary_score, 3),\n",
    "                'safety': round(safety_score, 3),\n",
    "                'execution': round(execution_score, 3),\n",
    "                'secondary_outcomes': round(secondary_score, 3)\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scoring trial: {e}\")\n",
    "        return {\n",
    "            'total_score': 0.5,\n",
    "            'components': {\n",
    "                'primary_outcome': 0.2,\n",
    "                'safety': 0.15,\n",
    "                'execution': 0.1,\n",
    "                'secondary_outcomes': 0.05\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06fb47f2-362a-4246-97fe-bca4e7efe04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_score': 0.659,\n",
       " 'components': {'primary_outcome': 0.4,\n",
       "  'safety': 0,\n",
       "  'execution': 0.159,\n",
       "  'secondary_outcomes': 0.1}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = get_clinical_trials()['studies'][9]\n",
    "score_primary_outcome(study)\n",
    "score_safety(study)\n",
    "score_study_execution(study)\n",
    "score_secondary_outcomes(study)\n",
    "score_clinical_trial(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6d14a-6283-4333-bbad-93e7bd59174a",
   "metadata": {},
   "source": [
    "## Filtering trials\n",
    "\n",
    "We drop all trials that aren't classed as 'INDUSTRY'.\n",
    "\n",
    "I use 0.8 as a minimum score for now, with the strong caveat that per above this algorithm would need serious refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9fdfcb56-5502-40da-a0ab-3f72864492e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trials = []\n",
    "for trial in all_trials:\n",
    "    is_company =  org_info(trial)['class'] == 'INDUSTRY'\n",
    "    score = score_clinical_trial(trial)['total_score'] \n",
    "    if is_company and score > 0.8:\n",
    "        filtered_trials.append(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d30598c1-2435-485f-a201-b7b3245b254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Tandem Diabetes Care, Inc.', '2023-01-05'],\n",
       " ['Boston Scientific Corporation', '2022-05-27'],\n",
       " ['Phathom Pharmaceuticals, Inc.', '2023-10-24'],\n",
       " ['GlaxoSmithKline', '2020-09-04'],\n",
       " ['Rhaeos, Inc.', '2021-08-10']]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successful_trials = []\n",
    "for trial in filtered_trials:\n",
    "    info = []\n",
    "    info.append(org_info(trial)['fullName'])\n",
    "    info.append(trial['protocolSection']['statusModule']['studyFirstSubmitDate'])\n",
    "    successful_trials.append(info)\n",
    "successful_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "dde4c06d-5bad-4d11-9cf0-7963ede1b01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fullName': 'Tandem Diabetes Care, Inc.', 'class': 'INDUSTRY'}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_info(filtered_trials[0])\n",
    "# filtered_trials[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
